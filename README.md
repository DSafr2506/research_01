# Изучение влияния различных функций активации и оптимизаторов на сходимость нейросетей при решении задачи классификации изображений на 200 классов

## Задача 
Классификация изображений на 200 классов



### Датасет 
- ImageNet-200 (уменьшенная версия ImageNet с 200 классами)

### используемые функции активации


### Базовые
- ReLU
- Swish
- GELU
### Адаптивные
- PAReLU
- Dynamic Swish
- Adaptive GELU
### Новые функции 
- Mish
- LiSHT
- ELU+
- SmeLU
- Aria-2

### Оптимизаторы
- SGD
- Adam
- AdamW
- LAMB (для больших batch)
- RAdam (исправленный Adam)
- Sophia 



### Архитектуры
- MLP-Mixer (современная MLP-архитектура)
- ResNet-18 (CNN)
- ViT-Tiny (упрощенный Vision Transformer)



### Возможные эксперементы для исследования 
- Обучение на low-resolution (64x64) и high-resolution (384x384)
- Несбалансированные классы
- составные активации
- использование batch разных размеров

